### SVM原理简介
* **SVM的目标函数**

SVM期望能找到最大超平面分割两类，分两个方面考虑支持向量机的目标函数。
（1）两个类之间的间隔最大化即最小化$$ \frac{1}{2}w^Tw $$。
（2）考虑错误分类的代价，对于$$y_i$$为正例$$ w^Tx \gt 0$$，对于$$y_i$$为负例 $$ w^Tx\lt0 $$，我们希望$$ w^Txy_i \gt 0$$。当$$ w^Txy_i$$为负或者小于1，带来了$$1-w^Txy_i$$的损失。
综合上述，SVM的损失函数加上模型的惩罚项和正则化。目标函数变为：

$$ L=\sum_{i=0}^n {max(0,1-w^Txy_i)}+Cw^Tw （1）$$。
其中的hinge Loss可以类比逻辑回归中的Log Loss。

（3）考虑目标函数为$$ \frac{1}{2}w^Tw $$，约束条件可以写为
$$ wx_i+b \ge 1,y_i=1 $$
$$wx_i+b\le-1,y_i=-1 $$
这样可以改写（2）中的目标函数为拉格朗日函数：
$$ L=-\sum_{i=0}^n {a_i(y_i(w^Tx+b)-1)}+\frac{1}{2}w^Tw （2）$$
求偏倒数可以得到：$$\frac{\partial L}{\partial x}=0 $$可以得到 
         $$ w=\sum_{i=0}^n a_iy_ix_i （3）$$
从而希望求出w的值，可以继续求偏倒数令 
   $$\sum_{i=0}^n a_iy_i=0$$，这样问题就变为一个线性规划问题。

（4）对于（2）式，可以考虑引入松弛变量解决错误分类问题。

$$ L=C\sum_{i=0}^n {\epsilon_i}+\frac{1}{2}w^Tw$$
$$s.t. y_i(wx_i+b) \ge 1-\epsilon_i $$

尽管SVM数学公式较多，但可以分几个方面进行考虑：（1）如何最大间隔不同类(2)如何惩罚错误的分类和不可分类;（3）如何解决稀疏数据;(4)如何进行线性规划。
* **SVM的核函数**

****对于线性不可分的模型，可以考虑核函数技巧Kernel Trick对数据进行转化，将低维的数据映射到高维度的空间中，转化为线性可分模型。一个核函数K(Kernel Function) 指的是 其中x，y是n维的输入值，$$k(x,y)=(f(x),f(y))$$是从n到m(m>>n)的映射，$$(x,y)$$是点乘,$$k(x,y)$$反应了 $$x$$ 和 $$y$$的距离,是一种距离度量的方法。
当将核函数从低维映射到高维时，计算量很大。Kernel Trick寻找快捷的方法进行计算，找到低维函数使得原数据线性可分。Kernel 函数需要满足半正定矩阵的特征。
常见的Kernel 函数有：高斯核函数、多项式核函数、线性核函数（PCA）等。

* **SVM的SMO算法**

SMO算法通过启发式方法选择两个$$α_i,α_j$$当变量，固定其他$$α_k$$，然后用解析的方法求解两个变量的二次规划问题。具体原理待叙述。

Kernel方法的参考文章：http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/#kernel_trick

