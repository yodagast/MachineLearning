## **Word2Vec知识点**

**\(1\)基本介绍**

Word2Vec是一种基本的词向量表示，通过降维将一个高维度的词向量变为低维度的向量表示。Word2Vec可以用来进行聚类、同义词分析、作为其他神经网络模型的输入层等。如果把词作为一个K维度的特征，那么就可以寻找句子、文章的更深层次特征表示。

**\(2\)词向量表示**

* one-hot 词向量表示。对于某个预料库中的词，可以表示为\[0,0,…,1,…,0\]的形式其中的1表示第i个单词在预料库中出现过。
* TF-IDF 词向量表示。对预料库中的词，第i个词可以被替换成这个词在文档中出现的TF\*IDF值，这和one-hot 的词向量模型相似。
* 除了基本的TF-IDF，LSA和LDA模型也是常见文本向量模型。
* Word2Vec 词嵌入。对于one-hot的词表示方法进行降维，将n维度的词向量转化为k维度的词向量进行知识表示（word represetation）。下面介绍如何学习低维度的词向量模型。

**\(3\)N-gram**

* CBOW。在自然语言的句子中，如“中国 的 首都 是 北京”预测第3个词语“北京”，可以结合语句的上下文进行预测。

$$ p(w_t|context)=p(w_t|w_{(t-k)}...w_{(t-1)},w_{(t+1)}...w_{(t+k)})$$

* Skip-Gram。和CBOW相反，Skip-Gram在模型预测中，基于第t个词预测上下文。skip-gram通过最大化文档的平均log 概率来进行模型预测。skip-gram采用$$p(w_{t+j}|w_t)$$来预测t+j个位置上的单词。对$$ p(w_{t+j}|w_t) $$采用softmax模型。

对于这两个模型的详细解释可以参照原paper进行阅读理解。下图有一个直观的解释。

![](/assets/import_CBOW.png)

**\(4\)Hierarchical Softmax**



